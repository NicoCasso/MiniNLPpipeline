{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69c19f98",
   "metadata": {},
   "source": [
    "tutorial reference : https://www.geeksforgeeks.org/machine-learning/text-preprocessing-in-python-set-1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c69a5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 1. Importing Libraries\n",
    "\n",
    "We will be importing nltk, regex, string and inflect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf27601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import inflect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd614c6b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 2. Convert to Lowercase\n",
    "\n",
    "We lowercase the text to reduce the size of the vocabulary of our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51420568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hey, did you know that the summer break is coming? amazing right !! it's only 5 more days !!\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "input_str = \"Hey, did you know that the summer break is coming? Amazing right !! It's only 5 more days !!\"\n",
    "\n",
    "text_lowercase(input_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f22534",
   "metadata": {},
   "source": [
    "# 3. Removing Numbers\n",
    "\n",
    "We can either remove numbers or convert the numbers into their textual representations. To remove the numbers we can use regular expressions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "451aa83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are  balls in this bag, and  in the other one.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_numbers(text):\n",
    "    result = re.sub(r'\\d+', '', text)\n",
    "    return result\n",
    "\n",
    "input_str = \"There are 3 balls in this bag, and 12 in the other one.\"\n",
    "\n",
    "remove_numbers(input_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207752af",
   "metadata": {},
   "source": [
    "# 4. Converting Numerical Values\n",
    "\n",
    "We can also convert the numbers into words. This can be done by using the inflect library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07e9050f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are three balls in this bag, and twelve in the other one.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = inflect.engine()\n",
    "\n",
    "def convert_number(text):\n",
    "    temp_str = text.split()\n",
    "    new_string = []\n",
    "    for word in temp_str:\n",
    "        if word.isdigit():\n",
    "            temp = p.number_to_words(word)\n",
    "            new_string.append(temp)\n",
    "        else:\n",
    "            new_string.append(word)\n",
    "\n",
    "    temp_str = ' '.join(new_string)\n",
    "    return temp_str\n",
    "\n",
    "input_str = 'There are 3 balls in this bag, and 12 in the other one.'\n",
    "\n",
    "convert_number(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899bdf5",
   "metadata": {},
   "source": [
    "# 5. Removing Punctuation\n",
    "\n",
    "We remove punctuations so that we don't have different forms of the same word. For example if we don't remove the punctuation then been. been, been! will be treated separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaeb9701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hey did you know that the summer break is coming Amazing right  Its only 5 more days '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "input_str = \"Hey, did you know that the summer break is coming? Amazing right !! It's only 5 more days !!\"\n",
    "\n",
    "remove_punctuation(input_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0f06c9",
   "metadata": {},
   "source": [
    "# 6. Removing Whitespace\n",
    "\n",
    "We can use the join and split function to remove all the white spaces in a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c01ee56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"we don't need the given questions\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    "\n",
    "input_str = \"we don't need   the given questions\"\n",
    "\n",
    "remove_whitespace(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f21702a",
   "metadata": {},
   "source": [
    "# 7. Removing Stopwords\n",
    "\n",
    "Stopwords are words that do not contribute much to the meaning of a sentence hence they can be removed. The NLTK library has a set of stopwords and we can use these to remove stopwords from our text. Below is the list of stopwords available in NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1949591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nicolascassonnet/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('punkt_tab') # is that usefull ?\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa38e0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'sample', 'sentence', 'going', 'remove', 'stopwords', '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords as sw\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(sw.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "example_text = \"This is a sample sentence and we are going to remove the stopwords from this.\"\n",
    "\n",
    "remove_stopwords(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6188c73f",
   "metadata": {},
   "source": [
    "# 8. Applying Stemming\n",
    "\n",
    "Stemming is the process of getting the root form of a word. Stem or root is the part to which affixes like -ed, -ize, -de, -s, etc are added. The stem of a word is created by removing the prefix or suffix of a word.\n",
    "\n",
    "Example: \n",
    "\n",
    "    books      --->    book\n",
    "    looked     --->    look\n",
    "    denied     --->    deni\n",
    "    flies      --->    fli\n",
    "\n",
    "There are mainly three algorithms for stemming. These are the Porter Stemmer, the Snowball Stemmer and the Lancaster Stemmer. Porter Stemmer is the most common among them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "934768cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'scienc',\n",
       " 'use',\n",
       " 'scientif',\n",
       " 'method',\n",
       " 'algorithm',\n",
       " 'and',\n",
       " 'mani',\n",
       " 'type',\n",
       " 'of',\n",
       " 'process']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stems = [stemmer.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "text = 'data science uses scientific methods algorithms and many types of processes'\n",
    "\n",
    "stem_words(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0b0878",
   "metadata": {},
   "source": [
    "# 9. Applying Lemmatization\n",
    "\n",
    "Lemmatization is a NLP technique that reduces a word to its root form. This can be helpful for tasks such as text analysis and search as it allows you to compare words that are related but have different forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0aa95a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/nicolascassonnet/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'science',\n",
       " 'us',\n",
       " 'scientific',\n",
       " 'method',\n",
       " 'algorithm',\n",
       " 'and',\n",
       " 'many',\n",
       " 'type',\n",
       " 'of',\n",
       " 'process']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemma_words(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "input_str = \"data science uses scientific methods algorithms and many types of processes\"\n",
    "\n",
    "lemma_words(input_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d466b149",
   "metadata": {},
   "source": [
    "\n",
    "In this guide we learned different NLP text preprocessing technique which can be used to make a NLP based application and project.\n",
    "\n",
    "Must Read:\n",
    "\n",
    "        Natural Language Processing (NLP) Tutorial https://www.geeksforgeeks.org/natural-language-processing-nlp-tutorial/\n",
    "        Phases of Natural Language Processing (NLP) https://www.geeksforgeeks.org/machine-learning/phases-of-natural-language-processing-nlp/\n",
    "        POS(Parts-Of-Speech) Tagging in NLP https://www.geeksforgeeks.org/nlp-part-of-speech-default-tagging/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
