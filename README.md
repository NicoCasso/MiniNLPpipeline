# Contexte du projet

En tant que data scientist, dans le cadre de leur montée en compétences, les apprenants sont amenés à explorer le traitement automatique du langage naturel (NLP). Le Module 2 de la formation leur a permis d’aborder concrètement les étapes de prétraitement des données textuelles, une phase essentielle avant toute tâche NLP.

Après avoir découvert le contexte général du NLP et ses défis dans le module précédent, ce second module leur propose une mise en pratique concrète des savoirs acquis :

Nettoyage des données : suppression des caractères spéciaux, mise en minuscule, normalisation Unicode, etc.

Tokenisation : découpage en mots ou en phrases.

Suppression des stopwords.

Lemmatisation et stemming.

Visualisation et vérification des transformations.

Les apprenants doivent mobiliser ces techniques pour transformer un corpus brut en une version "propre" et exploitable, prête à alimenter un modèle NLP supervisé.

Le projet met les étudiants dans une situation réaliste de traitement de texte en entreprise (analyse de retours clients, extraction de mots-clés, etc.).

# Modalités pédagogiques

Travail individuel ou en binôme.

Utilisation de Google Colab, Jupyter Notebook ou VSCode.

Intégration d’un jeu de données textuelles réel (ex. : commentaires clients, critiques de films, avis Google, tweets).

Présentation obligatoire d’une analyse réflexive à la fin du notebook (ce qui a été difficile, ce qui a été appris, limites du travail).

Accompagnement possible par le formateur à mi-parcours pour valider l’approche technique.
Modalités d'évaluation

L’évaluation repose sur :

La qualité technique du pipeline (fonctionnalité, structure, clarté du code).

La documentation (commentaires, structure du notebook, logique des étapes).

La cohérence des choix (ex. : pourquoi telle méthode de tokenisation ou de lemmatisation ?).

L’analyse réflexive en fin de projet.

Une présentation orale rapide (5-7 minutes par projet).

# Livrables

Un notebook ou script Python commenté, structuré et exécuté (Jupyter/Colab).

Un fichier texte ou markdown résumant les choix techniques (optionnel).

Une présentation orale ou vidéo enregistrée (selon le format choisi).

Lien vers le projet si hébergé sur GitHub ou Drive.

# Critères de performance

Le pipeline doit contenir au minimum :

1 étape de nettoyage (ponctuation, casse, etc.)

1 tokenisation

1 suppression de stopwords

1 lemmatisation ou stemming

Le code doit être fonctionnel et reproductible.

L’analyse réflexive doit montrer une prise de recul sur les étapes réalisées.

Le projet doit être terminé dans le temps imparti (1 semaine max).

Les fichiers doivent être propres, bien nommés et organisés.

# Ressources

Documentation NLTK https://www.nltk.org/
allociné https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews
preprocessing https://www.geeksforgeeks.org/machine-learning/text-preprocessing-in-python-set-1/

Documentation SpaCy https://spacy.io/
Intro NLP https://drlee.io/building-an-nlp-data-pipeline-and-eda-using-nltk-on-google-colab-268221712b28
data pipeline https://drlee.io/building-an-nlp-data-pipeline-and-eda-using-nltk-on-google-colab-268221712b28